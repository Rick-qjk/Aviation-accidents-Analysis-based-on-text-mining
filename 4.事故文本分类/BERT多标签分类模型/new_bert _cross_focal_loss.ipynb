{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import shutil\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.read_excel('Dataset_label_Augment.xlsx')\n",
    "df_final['target_list']=df_final['target_list'].apply(eval)\n",
    "\n",
    "def multilabel_train_test_split(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    多标签分层划分函数\n",
    "    \"\"\"\n",
    "    # 第一次划分：分离测试集\n",
    "    stratifier = IterativeStratification(n_splits=2, order=2,\n",
    "                                        sample_distribution_per_fold=[test_size, 1-test_size])\n",
    "    train_idx, test_idx = next(stratifier.split(X, y))\n",
    "    \n",
    "    # 第二次划分：从训练集分离验证集\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    stratifier = IterativeStratification(n_splits=2, order=2,\n",
    "                                        sample_distribution_per_fold=[val_ratio, 1-val_ratio])\n",
    "    tr_idx, val_idx = next(stratifier.split(X[train_idx], y[train_idx]))\n",
    "    \n",
    "    return X[train_idx][tr_idx], X[train_idx][val_idx], X[test_idx], \\\n",
    "           y[train_idx][tr_idx], y[train_idx][val_idx], y[test_idx]\n",
    "\n",
    "# 应用分层划分\n",
    "texts = df_final['clean_text'].values\n",
    "labels = np.array(df_final['target_list'].tolist())\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = multilabel_train_test_split(\n",
    "    texts, labels, test_size=0.1, val_size=0.1\n",
    ")\n",
    "\n",
    "# 重建DataFrame\n",
    "train_df = pd.DataFrame({'clean_text': X_train, 'target_list': list(y_train)})\n",
    "val_df = pd.DataFrame({'clean_text': X_val, 'target_list': list(y_val)})\n",
    "test_df = pd.DataFrame({'clean_text': X_test, 'target_list': list(y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_ratios(df, label_column='target_list'):\n",
    " \n",
    "    # 将标签列表转换为二维数组\n",
    "    labels_array = np.array(df[label_column].tolist())\n",
    "    \n",
    "    # 验证标签维度\n",
    "    assert labels_array.shape[1] == 34, f\"应为34个标签，实际检测到 {labels_array.shape[1]} 个\"\n",
    "    \n",
    "    # 计算每个标签的正样本比例\n",
    "    positive_ratios = labels_array.mean(axis=0)\n",
    "    \n",
    "    # 转换为带标签的Series\n",
    "    return pd.Series(positive_ratios, \n",
    "                   index=[f'Label_{i}' for i in range(34)],\n",
    "                   name='Positive_Ratio')\n",
    "\n",
    "\n",
    "ratio_series = calculate_positive_ratios(train_df)\n",
    "\n",
    "alpha_series_clipped = (1 - ratio_series).clip(upper=0.99)\n",
    "\n",
    "alpha_weights=alpha_series_clipped.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha_per_class, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        多标签Focal Loss\n",
    "        \n",
    "        参数:\n",
    "        alpha_per_class: 每个类别的平衡因子α，形状为[num_classes]的Tensor或list\n",
    "        gamma: 聚焦参数γ（默认2.0）\n",
    "        reduction: 损失聚合方式 ('mean'或'sum')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 转换alpha为张量\n",
    "        if not isinstance(alpha_per_class, torch.Tensor):\n",
    "            alpha_per_class = torch.tensor(alpha_per_class)\n",
    "        \n",
    "        self.alpha = alpha_per_class  # 形状 [num_classes]\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = 1e-8  # 防止数值不稳定\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        前向计算\n",
    "        \n",
    "        参数:\n",
    "        inputs: 模型输出logits，形状 [batch_size, num_classes]\n",
    "        targets: 真实标签，形状 [batch_size, num_classes]，值在0-1之间\n",
    "        \"\"\"\n",
    "        # 计算基础交叉熵损失（自动应用sigmoid）\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none')  # 形状 [batch_size, num_classes]\n",
    "\n",
    "        # 计算p_t = exp(-bce_loss)\n",
    "        p_t = torch.exp(-bce_loss)  # 形状与bce_loss相同\n",
    "        \n",
    "        # 计算alpha因子\n",
    "        alpha_t = self.alpha.to(device=targets.device)  # 确保在正确设备上\n",
    "        alpha_t = alpha_t[None, :]  # 扩展维度到 [1, num_classes]\n",
    "\n",
    "        # 计算Focal Loss\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "\n",
    "        # 聚合损失\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, df, max_len=512):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"autodl-tmp/bert-base-uncased\")\n",
    "        self.texts = df['clean_text']\n",
    "        self.labels = df['target_list']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label=self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'        \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.float)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mydataset(train_df)\n",
    "val_dataset = Mydataset(val_df)\n",
    "test_dataset=Mydataset(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "class MyBert(nn.Module):\n",
    "    def __init__(self,num_labels):\n",
    "        super(MyBert,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('autodl-tmp/bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "   \n",
    "    def forward(self, input_ids, attention_mask):      \n",
    "        outputs = self.bert(input_ids, attention_mask)     \n",
    "        pooled_output = outputs.pooler_output  \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyBert(34)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "if gpu_count > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_model(model)\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  \n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.save_model(model)\n",
    "            self.counter = 0  \n",
    "\n",
    "    def save_model(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f\"Validation loss improved. Model saved to {self.path}\")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, path='best_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha_per_class, gamma=2.0, reduction='mean'):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 转换alpha为张量\n",
    "        if not isinstance(alpha_per_class, torch.Tensor):\n",
    "            alpha_per_class = torch.tensor(alpha_per_class)\n",
    "        \n",
    "        self.alpha = alpha_per_class  # 形状 [num_classes]\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = 1e-8  # 防止数值不稳定\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        # 计算基础交叉熵损失（自动应用sigmoid）\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none')  # 形状 [batch_size, num_classes]\n",
    "\n",
    "        # 计算p_t = exp(-bce_loss)\n",
    "        p_t = torch.exp(-bce_loss)  # 形状与bce_loss相同\n",
    "        \n",
    "        # 计算alpha因子\n",
    "        alpha_t = self.alpha.to(device=targets.device)  # 确保在正确设备上\n",
    "        alpha_t = alpha_t[None, :]  # 扩展维度到 [1, num_classes]\n",
    "\n",
    "        # 计算Focal Loss\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "\n",
    "        # 聚合损失\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha_per_class=alpha_weights, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BATCH: 0, Training Loss:  0.7028512358665466\n",
      "Epoch: 1, BATCH: 100, Training Loss:  0.3356117904186249\n",
      "Epoch: 1, BATCH: 200, Training Loss:  0.2522735297679901\n",
      "Epoch: 1, BATCH: 300, Training Loss:  0.23229444026947021\n",
      "Epoch: 1, BATCH: 400, Training Loss:  0.21668347716331482\n",
      "Epoch: 1, BATCH: 500, Training Loss:  0.2033631056547165\n",
      "Epoch: 1, BATCH: 600, Training Loss:  0.20739732682704926\n",
      "Epoch 1: Training End\n",
      "Epoch 1: Validation Start\n",
      "Epoch 1, Training Loss: 0.25842787980858534\n",
      "Validation Loss: 0.18444907480908423\n",
      "Validation Accuracy: 0.01458198314970836\n",
      "Validation F1 Score: 0.06333792272657812\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 2, BATCH: 0, Training Loss:  0.19035185873508453\n",
      "Epoch: 2, BATCH: 100, Training Loss:  0.18367676436901093\n",
      "Epoch: 2, BATCH: 200, Training Loss:  0.15802277624607086\n",
      "Epoch: 2, BATCH: 300, Training Loss:  0.1671716272830963\n",
      "Epoch: 2, BATCH: 400, Training Loss:  0.1730836182832718\n",
      "Epoch: 2, BATCH: 500, Training Loss:  0.16090692579746246\n",
      "Epoch: 2, BATCH: 600, Training Loss:  0.16485150158405304\n",
      "Epoch 2: Training End\n",
      "Epoch 2: Validation Start\n",
      "Epoch 2, Training Loss: 0.17027734886428827\n",
      "Validation Loss: 0.15145615495971798\n",
      "Validation Accuracy: 0.07145171743357097\n",
      "Validation F1 Score: 0.14920700739032383\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 3, BATCH: 0, Training Loss:  0.15836778283119202\n",
      "Epoch: 3, BATCH: 100, Training Loss:  0.15400785207748413\n",
      "Epoch: 3, BATCH: 200, Training Loss:  0.15136873722076416\n",
      "Epoch: 3, BATCH: 300, Training Loss:  0.1369699090719223\n",
      "Epoch: 3, BATCH: 400, Training Loss:  0.14099982380867004\n",
      "Epoch: 3, BATCH: 500, Training Loss:  0.11625780910253525\n",
      "Epoch: 3, BATCH: 600, Training Loss:  0.12410349398851395\n",
      "Epoch 3: Training End\n",
      "Epoch 3: Validation Start\n",
      "Epoch 3, Training Loss: 0.14025192424829755\n",
      "Validation Loss: 0.13300832453155026\n",
      "Validation Accuracy: 0.12670123136746597\n",
      "Validation F1 Score: 0.25748862454337995\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 4, BATCH: 0, Training Loss:  0.11960917711257935\n",
      "Epoch: 4, BATCH: 100, Training Loss:  0.12176003307104111\n",
      "Epoch: 4, BATCH: 200, Training Loss:  0.1217820793390274\n",
      "Epoch: 4, BATCH: 300, Training Loss:  0.11716947704553604\n",
      "Epoch: 4, BATCH: 400, Training Loss:  0.11382827907800674\n",
      "Epoch: 4, BATCH: 500, Training Loss:  0.12672433257102966\n",
      "Epoch: 4, BATCH: 600, Training Loss:  0.11262346804141998\n",
      "Epoch 4: Training End\n",
      "Epoch 4: Validation Start\n",
      "Epoch 4, Training Loss: 0.12005640776101445\n",
      "Validation Loss: 0.11971605315650861\n",
      "Validation Accuracy: 0.18081659105638367\n",
      "Validation F1 Score: 0.3433275048402637\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 5, BATCH: 0, Training Loss:  0.12270691990852356\n",
      "Epoch: 5, BATCH: 100, Training Loss:  0.12087556719779968\n",
      "Epoch: 5, BATCH: 200, Training Loss:  0.10041678696870804\n",
      "Epoch: 5, BATCH: 300, Training Loss:  0.1052047535777092\n",
      "Epoch: 5, BATCH: 400, Training Loss:  0.10182017087936401\n",
      "Epoch: 5, BATCH: 500, Training Loss:  0.10970359295606613\n",
      "Epoch: 5, BATCH: 600, Training Loss:  0.10560555011034012\n",
      "Epoch 5: Training End\n",
      "Epoch 5: Validation Start\n",
      "Epoch 5, Training Loss: 0.10500777749852701\n",
      "Validation Loss: 0.11033418367511218\n",
      "Validation Accuracy: 0.23379779650032403\n",
      "Validation F1 Score: 0.46569109167316064\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 6, BATCH: 0, Training Loss:  0.093526192009449\n",
      "Epoch: 6, BATCH: 100, Training Loss:  0.09911125153303146\n",
      "Epoch: 6, BATCH: 200, Training Loss:  0.10615933686494827\n",
      "Epoch: 6, BATCH: 300, Training Loss:  0.08952946960926056\n",
      "Epoch: 6, BATCH: 400, Training Loss:  0.09544654935598373\n",
      "Epoch: 6, BATCH: 500, Training Loss:  0.09478457272052765\n",
      "Epoch: 6, BATCH: 600, Training Loss:  0.08431489020586014\n",
      "Epoch 6: Training End\n",
      "Epoch 6: Validation Start\n",
      "Epoch 6, Training Loss: 0.09252650751745946\n",
      "Validation Loss: 0.10425437326283798\n",
      "Validation Accuracy: 0.2639338950097213\n",
      "Validation F1 Score: 0.5403525846891885\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 7, BATCH: 0, Training Loss:  0.08824452012777328\n",
      "Epoch: 7, BATCH: 100, Training Loss:  0.09515009075403214\n",
      "Epoch: 7, BATCH: 200, Training Loss:  0.07838303595781326\n",
      "Epoch: 7, BATCH: 300, Training Loss:  0.07671821862459183\n",
      "Epoch: 7, BATCH: 400, Training Loss:  0.07808098942041397\n",
      "Epoch: 7, BATCH: 500, Training Loss:  0.087431401014328\n",
      "Epoch: 7, BATCH: 600, Training Loss:  0.07793575525283813\n",
      "Epoch 7: Training End\n",
      "Epoch 7: Validation Start\n",
      "Epoch 7, Training Loss: 0.08171582255304909\n",
      "Validation Loss: 0.09955427857096662\n",
      "Validation Accuracy: 0.27138690861957226\n",
      "Validation F1 Score: 0.5812443338094776\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 8, BATCH: 0, Training Loss:  0.06706324964761734\n",
      "Epoch: 8, BATCH: 100, Training Loss:  0.06662792712450027\n",
      "Epoch: 8, BATCH: 200, Training Loss:  0.07202424854040146\n",
      "Epoch: 8, BATCH: 300, Training Loss:  0.0794840008020401\n",
      "Epoch: 8, BATCH: 400, Training Loss:  0.07385281473398209\n",
      "Epoch: 8, BATCH: 500, Training Loss:  0.06164921075105667\n",
      "Epoch: 8, BATCH: 600, Training Loss:  0.07641494274139404\n",
      "Epoch 8: Training End\n",
      "Epoch 8: Validation Start\n",
      "Epoch 8, Training Loss: 0.07187586945216677\n",
      "Validation Loss: 0.09364320145747096\n",
      "Validation Accuracy: 0.32193778353856123\n",
      "Validation F1 Score: 0.6430962381903846\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 9, BATCH: 0, Training Loss:  0.05924304202198982\n",
      "Epoch: 9, BATCH: 100, Training Loss:  0.05750935897231102\n",
      "Epoch: 9, BATCH: 200, Training Loss:  0.06468048691749573\n",
      "Epoch: 9, BATCH: 300, Training Loss:  0.0616481713950634\n",
      "Epoch: 9, BATCH: 400, Training Loss:  0.05829499661922455\n",
      "Epoch: 9, BATCH: 500, Training Loss:  0.049141280353069305\n",
      "Epoch: 9, BATCH: 600, Training Loss:  0.055681679397821426\n",
      "Epoch 9: Training End\n",
      "Epoch 9: Validation Start\n",
      "Epoch 9, Training Loss: 0.06280238000550403\n",
      "Validation Loss: 0.09058790415832677\n",
      "Validation Accuracy: 0.3460790667530784\n",
      "Validation F1 Score: 0.6959634347687835\n",
      "Validation loss improved. Model saved to best_model\n",
      "Epoch: 10, BATCH: 0, Training Loss:  0.059314362704753876\n",
      "Epoch: 10, BATCH: 100, Training Loss:  0.05817196145653725\n",
      "Epoch: 10, BATCH: 200, Training Loss:  0.057116881012916565\n",
      "Epoch: 10, BATCH: 300, Training Loss:  0.05621914193034172\n",
      "Epoch: 10, BATCH: 400, Training Loss:  0.05366245284676552\n",
      "Epoch: 10, BATCH: 500, Training Loss:  0.049547068774700165\n",
      "Epoch: 10, BATCH: 600, Training Loss:  0.05464691296219826\n",
      "Epoch 10: Training End\n",
      "Epoch 10: Validation Start\n",
      "Epoch 10, Training Loss: 0.05475671860850976\n",
      "Validation Loss: 0.0877082932778855\n",
      "Validation Accuracy: 0.3694102397926118\n",
      "Validation F1 Score: 0.7367423741630751\n",
      "Validation loss improved. Model saved to best_model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "num_epochs = 16\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        input_ids=batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits=model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        loss=criterion(logits,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, BATCH: {batch_idx}, Training Loss:  {loss.item()}')\n",
    "    \n",
    "    print('Epoch {}: Training End'.format(epoch+1))\n",
    "    print('Epoch {}: Validation Start'.format(epoch+1))\n",
    "\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], [] \n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in val_loader:  \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float() \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "  \n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() \n",
    "            probs = torch.sigmoid(logits) \n",
    "            preds = (probs > 0.5).int()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation F1 Score: {f1}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1777/3396049628.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing\n",
      "\n",
      "Final Test Evaluation:\n",
      "Hamming Loss: 0.0339\n",
      "Micro-F1: 0.7394\n",
      "Macro-F1: 0.7075\n",
      "AUC-PR (Macro): 0.7785\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ADRM       0.98      0.55      0.70        80\n",
      "        AMAN       0.92      0.12      0.22       375\n",
      "         ARC       0.80      0.71      0.75       893\n",
      "         ATM       0.97      0.74      0.84        86\n",
      "        BIRD       0.99      0.92      0.95        95\n",
      "       CABIN       1.00      0.11      0.20        98\n",
      "        CFIT       0.73      0.59      0.65       387\n",
      "        CTOL       0.74      0.80      0.77      3212\n",
      "        EVAC       0.95      0.73      0.82       135\n",
      "        EXTL       0.92      0.39      0.55       157\n",
      "        F-NI       0.94      0.89      0.91       201\n",
      "      F-POST       0.86      0.32      0.47       386\n",
      "        FUEL       0.93      0.83      0.88       697\n",
      "        GCOL       0.89      0.72      0.80       269\n",
      "        GTOW       0.96      0.93      0.95       112\n",
      "         ICE       0.90      0.85      0.87       119\n",
      "        LALT       0.70      0.54      0.61       298\n",
      "       LOC-G       0.71      0.74      0.72       654\n",
      "       LOC-I       0.84      0.73      0.78      1521\n",
      "        LOLI       0.71      0.52      0.60       223\n",
      "         MAC       0.96      0.92      0.94       219\n",
      "      OTHERS       0.56      0.25      0.34       635\n",
      "        RAMP       0.78      0.24      0.36       429\n",
      "          RE       0.75      0.71      0.73      1506\n",
      "          RI       0.98      0.80      0.88        60\n",
      "      SCF-NP       0.83      0.68      0.75       543\n",
      "      SCF-PP       0.84      0.95      0.89      1041\n",
      "         SEC       0.93      0.43      0.59        30\n",
      "        TURB       0.90      0.85      0.87       203\n",
      "        UIMC       0.82      0.74      0.78       358\n",
      "         UNK       1.00      0.53      0.69        19\n",
      "        USOS       0.87      0.66      0.75       394\n",
      "        WILD       0.70      0.85      0.77        60\n",
      "       WSTRW       0.92      0.53      0.67       184\n",
      "\n",
      "   micro avg       0.80      0.69      0.74     15679\n",
      "   macro avg       0.86      0.64      0.71     15679\n",
      "weighted avg       0.80      0.69      0.72     15679\n",
      " samples avg       0.79      0.69      0.72     15679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"Loaded best model for testing\")\n",
    "\n",
    "# 添加测试评估部分\n",
    "from sklearn.metrics import hamming_loss, f1_score, average_precision_score\n",
    "\n",
    "model.eval()\n",
    "test_probs = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "test_probs = np.concatenate(test_probs)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "\n",
    "# 计算评估指标\n",
    "hl = hamming_loss(test_labels, test_preds)\n",
    "micro_f1 = f1_score(test_labels, test_preds, average='micro')\n",
    "macro_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "auc_pr = average_precision_score(test_labels, test_probs, average='macro')\n",
    "\n",
    "print(\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Hamming Loss: {hl:.4f}\")\n",
    "print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"AUC-PR (Macro): {auc_pr:.4f}\")\n",
    "\n",
    "label_columns=['ADRM','AMAN', 'ARC', 'ATM', 'BIRD', 'CABIN','CFIT', 'CTOL', 'EVAC', 'EXTL', 'F-NI', 'F-POST', 'FUEL',\n",
    "'GCOL',  'GTOW',  'ICE',  'LALT',  'LOC-G',  'LOC-I', 'LOLI', 'MAC', 'OTHERS','RAMP', 'RE',\n",
    "'RI',  'SCF-NP',  'SCF-PP',  'SEC',  'TURB',  'UIMC',  'UNK', 'USOS','WILD', 'WSTRW']\n",
    "# 可选：输出分类报告\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=label_columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import shutil\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.read_excel('Dataset_label_Augment.xlsx')\n",
    "df_final['target_list']=df_final['target_list'].apply(eval)\n",
    "\n",
    "def multilabel_train_test_split(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    多标签分层划分函数\n",
    "    \"\"\"\n",
    "    # 第一次划分：分离测试集\n",
    "    stratifier = IterativeStratification(n_splits=2, order=2,\n",
    "                                        sample_distribution_per_fold=[test_size, 1-test_size])\n",
    "    train_idx, test_idx = next(stratifier.split(X, y))\n",
    "    \n",
    "    # 第二次划分：从训练集分离验证集\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    stratifier = IterativeStratification(n_splits=2, order=2,\n",
    "                                        sample_distribution_per_fold=[val_ratio, 1-val_ratio])\n",
    "    tr_idx, val_idx = next(stratifier.split(X[train_idx], y[train_idx]))\n",
    "    \n",
    "    return X[train_idx][tr_idx], X[train_idx][val_idx], X[test_idx], \\\n",
    "           y[train_idx][tr_idx], y[train_idx][val_idx], y[test_idx]\n",
    "\n",
    "# 应用分层划分\n",
    "texts = df_final['clean_text'].values\n",
    "labels = np.array(df_final['target_list'].tolist())\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = multilabel_train_test_split(\n",
    "    texts, labels, test_size=0.1, val_size=0.1\n",
    ")\n",
    "\n",
    "# 重建DataFrame\n",
    "train_df = pd.DataFrame({'clean_text': X_train, 'target_list': list(y_train)})\n",
    "val_df = pd.DataFrame({'clean_text': X_val, 'target_list': list(y_val)})\n",
    "test_df = pd.DataFrame({'clean_text': X_test, 'target_list': list(y_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_ratios(df, label_column='target_list'):\n",
    " \n",
    "    # 将标签列表转换为二维数组\n",
    "    labels_array = np.array(df[label_column].tolist())\n",
    "    \n",
    "    # 验证标签维度\n",
    "    assert labels_array.shape[1] == 34, f\"应为34个标签，实际检测到 {labels_array.shape[1]} 个\"\n",
    "    \n",
    "    # 计算每个标签的正样本比例\n",
    "    positive_ratios = labels_array.mean(axis=0)\n",
    "    \n",
    "    # 转换为带标签的Series\n",
    "    return pd.Series(positive_ratios, \n",
    "                   index=[f'Label_{i}' for i in range(34)],\n",
    "                   name='Positive_Ratio')\n",
    "\n",
    "\n",
    "ratio_series = calculate_positive_ratios(train_df)\n",
    "\n",
    "alpha_series_clipped = (1 - ratio_series).clip(upper=0.99)\n",
    "\n",
    "alpha_weights=alpha_series_clipped.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, df, max_len=512):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"autodl-tmp/bert-base-uncased\")\n",
    "        self.texts = df['clean_text']\n",
    "        self.labels = df['target_list']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label=self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'        \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.float)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mydataset(train_df)\n",
    "val_dataset = Mydataset(val_df)\n",
    "test_dataset=Mydataset(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "class MyBert(nn.Module):\n",
    "    def __init__(self,num_labels):\n",
    "        super(MyBert,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('autodl-tmp/bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "   \n",
    "    def forward(self, input_ids, attention_mask):      \n",
    "        outputs = self.bert(input_ids, attention_mask)     \n",
    "        pooled_output = outputs.pooler_output  \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyBert(34)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "if gpu_count > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_model(model)\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  \n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.save_model(model)\n",
    "            self.counter = 0  \n",
    "\n",
    "    def save_model(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f\"Validation loss improved. Model saved to {self.path}\")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, path='best_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha_per_class, gamma=2.0, reduction='mean'):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        if not isinstance(alpha_per_class, torch.Tensor):\n",
    "            alpha_per_class = torch.tensor(alpha_per_class)\n",
    "        \n",
    "        self.alpha = alpha_per_class  \n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = 1e-8  \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none')  \n",
    "\n",
    "        p_t = torch.exp(-bce_loss)   \n",
    "        alpha_t = self.alpha.to(device=targets.device)  \n",
    "        alpha_t = alpha_t[None, :]  \n",
    "    \n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha_per_class=alpha_weights, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "num_epochs = 16\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        input_ids=batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits=model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        loss=criterion(logits,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, BATCH: {batch_idx}, Training Loss:  {loss.item()}')\n",
    "    \n",
    "    print('Epoch {}: Training End'.format(epoch+1))\n",
    "    print('Epoch {}: Validation Start'.format(epoch+1))\n",
    "\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], [] \n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in val_loader:  \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float() \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "  \n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() \n",
    "            probs = torch.sigmoid(logits) \n",
    "            preds = (probs > 0.5).int()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation F1 Score: {f1}\")\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"Loaded best model for testing\")\n",
    "\n",
    "# 添加测试评估部分\n",
    "from sklearn.metrics import hamming_loss, f1_score, average_precision_score\n",
    "\n",
    "model.eval()\n",
    "test_probs = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "test_probs = np.concatenate(test_probs)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "\n",
    "# 计算评估指标\n",
    "hl = hamming_loss(test_labels, test_preds)\n",
    "micro_f1 = f1_score(test_labels, test_preds, average='micro')\n",
    "macro_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "auc_pr = average_precision_score(test_labels, test_probs, average='macro')\n",
    "\n",
    "print(\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Hamming Loss: {hl:.4f}\")\n",
    "print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"AUC-PR (Macro): {auc_pr:.4f}\")\n",
    "\n",
    "label_columns=['ADRM','AMAN', 'ARC', 'ATM', 'BIRD', 'CABIN','CFIT', 'CTOL', 'EVAC', 'EXTL', 'F-NI', 'F-POST', 'FUEL',\n",
    "'GCOL',  'GTOW',  'ICE',  'LALT',  'LOC-G',  'LOC-I', 'LOLI', 'MAC', 'OTHERS','RAMP', 'RE',\n",
    "'RI',  'SCF-NP',  'SCF-PP',  'SEC',  'TURB',  'UIMC',  'UNK', 'USOS','WILD', 'WSTRW']\n",
    "# 可选：输出分类报告\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=label_columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
